---
title: "Pre-processing Tweets"
author: "Pulak Jain 20BRS1126"
date: "2023-02-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("tm")  # for text mining
#install.packages("SnowballC") # for text stemming
#install.packages("wordcloud") # word-cloud generator 
#install.packages("RColorBrewer") # color palettes
#install.packages("syuzhet") # for sentiment analysis
#install.packages("ggplot2") # for plotting graphs
#install.packages("corpus")

```

```{r}
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
library(corpus)

```

```{r}
text=read.csv("D:\\EDA_Tweets\\sentiment_tweets_1.6M.csv")
```

```{r}
columns=c("Labels","ID's","Timestamp","Query","Username","Tweets")
for(i in seq(6)){
  colnames(text)[i]=columns[i]
}
write.csv(text, "D:\\EDA_Tweets\\Revised_sentiment_tweets_1.6M.csv")
```

```{r}
head(text)
```

```{r}
shuffled=text[sample(nrow(text):1),c("Labels","Tweets")]
df=shuffled[sample(1:nrow(text),10000),]
dim(df)
```



```{r warning=TRUE}
#install.packages('pander')
library(pander)
library(dplyr)
corpus0 = df %>% select(Labels,Tweets)
corpus0$Tweets = iconv(corpus0$Tweets,'UTF-8','ASCII')
corpus0 = as.data.frame((corpus0))
corpus0= na.omit(corpus0)
corpus=corpus0
pander(table(corpus0$Labels))
```


```{r}
textDoc = (VectorSource(corpus$Tweets))
```

```{r}
textDoc= Corpus(textDoc)
```

```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc <- tm_map(textDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")


```

# Convert the text to lower case

```{r}
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
```

# Remove numbers

```{r}
TextDoc <- tm_map(TextDoc, removeNumbers)
```

# Remove english common stopwords

```{r}
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
```

# Remove your own stop word

```{r}
TextDoc <- tm_map(TextDoc, removeWords, c("s", "company", "team")) 
```

# Remove punctuations

```{r}
TextDoc <- tm_map(TextDoc, removePunctuation)
```

# Eliminate extra white spaces

```{r}
TextDoc <- tm_map(TextDoc, stripWhitespace)
```

# Text stemming - which reduces words to their root form

```{r}
TextDoc <- tm_map(TextDoc, stemDocument)
```


```{r}

# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)

```


```{r}
dtm_m <- as.matrix(TextDoc_dtm)

# Sort by descending value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)


# Display the top 5 most frequent words
head(dtm_d, 5)
```



```{r}
#write.csv(dtm_d, "D:\\EDA_Tweets\\frequency.csv", row.names=FALSE)
```


```{r}
#install.packages("caret")
library(caret)

set.seed(1223)
train_index=createDataPartition(corpus$Labels,p=0.70,list=FALSE)

train1 = corpus[train_index]
test1 = corpus[-train_index]


train2 = TextDoc[train_index]
test2=  TextDoc[-train_index]
```

```{r}
#memory.limit(size=100000)
```


```{r}
dict = findFreqTerms(TextDoc_dtm,lowfreq=10)
news_train = TermDocumentMatrix(train2,list(dictionary=dict))
news_test = TermDocumentMatrix(test2,list(dictionary=dict))
```


```{r}

convert_counts = function(x){
  x = ifelse(x>0,1,0)
}

news_train = news_train %>% apply(MARGIN =2 ,FUN =convert_counts)
news_test = news_test %>% apply(MARGIN =2 ,FUN =convert_counts)

news_train = as.data.frame(news_train)
news_test = as.data.frame(news_test)


```


```{r}
write.csv(news_train, "D:\\EDA_Tweets\\news_train_1000.csv", row.names=TRUE)
write.csv(news_test, "D:\\EDA_Tweets\\news_test_1000.csv", row.names=TRUE)

```

```{r}
#memory.limit(size=56000)
```


```{r}
news_train=read.csv("D:\\EDA_Tweets\\news_train_transposed_1000.csv")
news_test=read.csv("D:\\EDA_Tweets\\news_test_transposed_1000.csv")
```



```{r}
head(news_train)
```




```{r}
news_train1x = cbind(news_train,Labels =factor(train1))
news_train1x=news_train1x[,-1]
head(news_train1x)
```



```{r}
library(caTools)
set.seed(123)
split = sample.split(news_train1x, SplitRatio = 0.80)
 
training_set = subset(news_train1x, split == TRUE)
test_set = subset(news_train1x, split == FALSE)
```



#SVM

```{r}
install.packages("e1071")
library(e1071)
svm_classifier=svm(Labels~.,data=news_train1x)
```


```{r}

news_test=news_test[,-1]
```


```{r}
y_pred= predict(svm_classifier,news_test1x)
```


```{r}
cm=confusionMatrix(y_pred,news_train1x$Labels)
cm
```


# KNN


```{r}
#install.packages("caTools")
#install.packages("class")
library(caTools)
library(class)
```


```{r}
classifier_knn = knn(Labels~.,data=news_train1x,k=2)
```






```{r}
install.packages("keras")
install.packages("mlbench")
install.packages("neuralnet")
install.packages("magrittr")
```



```{r}
library(keras)
library(mlbench)
library(dplyr)
library(magrittr)
library(neuralnet)
```



```{r}
n <- neuralnet(Labels ~ .,data = news_train1x,hidden = c(12,7),linear.output = F,lifesign = 'full', rep=1)
```
```{r}

```

