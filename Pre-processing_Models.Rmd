---
title: "Pre-processing Tweets"
author: "Pulak Jain 20BRS1126"
date: "2023-02-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#install.packages("tm")  # for text mining
#install.packages("SnowballC") # for text stemming
#install.packages("wordcloud") # word-cloud generator 
#install.packages("RColorBrewer") # color palettes
#install.packages("syuzhet") # for sentiment analysis
#install.packages("ggplot2") # for plotting graphs
#install.packages("corpus")

```

```{r}
library("tm")
library("SnowballC")
library("wordcloud")
library("RColorBrewer")
library("syuzhet")
library("ggplot2")
library(corpus)

```

```{r}
text=read.csv("D:\\EDA_Tweets\\sentiment_tweets_1.6M.csv")
```

```{r}
columns=c("Labels","ID's","Timestamp","Query","Username","Tweets")
for(i in seq(6)){
  colnames(text)[i]=columns[i]
}
write.csv(text, "D:\\EDA_Tweets\\Revised_sentiment_tweets_1.6M.csv")
```

```{r}
head(text)
dim(text)
```

```{r}
shuffled=text[sample(nrow(text):1),c("Labels","Tweets")]
df=shuffled[sample(1:nrow(text),25000),]
dim(df)
```



```{r warning=TRUE}
#install.packages('pander')
library(pander)
library(dplyr)
corpus0 = df %>% select(Labels,Tweets)
corpus0$Tweets = iconv(corpus0$Tweets,'UTF-8','ASCII')
corpus0 = as.data.frame((corpus0))
corpus0= na.omit(corpus0)
corpus=corpus0
pander(table(corpus0$Labels))
```


```{r}
textDoc = (VectorSource(corpus$Tweets))
```

```{r}
textDoc= Corpus(textDoc)
```

```{r}
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
TextDoc <- tm_map(textDoc, toSpace, "/")
TextDoc <- tm_map(TextDoc, toSpace, "@")
TextDoc <- tm_map(TextDoc, toSpace, "\\|")


```

# Convert the text to lower case

```{r}
TextDoc <- tm_map(TextDoc, content_transformer(tolower))
```

# Remove numbers

```{r}
TextDoc <- tm_map(TextDoc, removeNumbers)
```

# Remove english common stopwords

```{r}
TextDoc <- tm_map(TextDoc, removeWords, stopwords("english"))
```

# Remove your own stop word

```{r}
TextDoc <- tm_map(TextDoc, removeWords, c("s", "company", "team")) 
```

# Remove punctuations

```{r}
TextDoc <- tm_map(TextDoc, removePunctuation)
```

# Eliminate extra white spaces

```{r}
TextDoc <- tm_map(TextDoc, stripWhitespace)
```

# Text stemming - which reduces words to their root form

```{r}
TextDoc <- tm_map(TextDoc, stemDocument)
```


```{r}

# Build a term-document matrix
TextDoc_dtm <- TermDocumentMatrix(TextDoc)

```


```{r}
memory.limit(size=90000)
```


```{r}
dtm_m <- as.matrix(TextDoc_dtm)

# Sort by descending value of frequency
dtm_v <- sort(rowSums(dtm_m),decreasing=TRUE)
dtm_d <- data.frame(word = names(dtm_v),freq=dtm_v)


# Display the top 5 most frequent words
head(dtm_d, 5)
```



```{r}
#write.csv(dtm_d, "D:\\EDA_Tweets\\frequency.csv", row.names=FALSE)
```


```{r}
#install.packages("caret")
library(caret)

set.seed(1223)
train_index=createDataPartition(corpus$Labels,p=0.70,list=FALSE)

train1 = corpus[train_index]
test1 = corpus[-train_index]


train2 = TextDoc[train_index]
test2=  TextDoc[-train_index]
```


```{r}
dict = findFreqTerms(TextDoc_dtm,lowfreq=10)
news_train = TermDocumentMatrix(train2,list(dictionary=dict))
news_test = TermDocumentMatrix(test2,list(dictionary=dict))
```


```{r}

convert_counts = function(x){
  x = ifelse(x>0,1,0)
}

news_train = news_train %>% apply(MARGIN =2 ,FUN =convert_counts)
news_test = news_test %>% apply(MARGIN =2 ,FUN =convert_counts)

news_train = as.data.frame(news_train)
news_test = as.data.frame(news_test)


```


```{r}
write.csv(news_train, "D:\\EDA_Tweets\\news_train_25000.csv", row.names=TRUE)
write.csv(news_test, "D:\\EDA_Tweets\\news_test_25000.csv", row.names=TRUE)
write.csv(train1, "D:\\EDA_Tweets\\train1.csv", row.names=TRUE)
write.csv(test1, "D:\\EDA_Tweets\\test1.csv", row.names=TRUE)

```


```{r}
news_train=read.csv("D:\\EDA_Tweets\\news_train_transposed_25000.csv")
news_test=read.csv("D:\\EDA_Tweets\\news_test_transposed_25000.csv")
train1=read.csv("D:\\EDA_Tweets\\train1.csv")
test1=read.csv("D:\\EDA_Tweets\\test1.csv")

```




```{r}
train1 = train1$x
```




```{r}
news_train1x = cbind(news_train,Labels = factor(train1))
news_train1x=news_train1x[,-1]
head(news_train1x)
```



```{r}
library(caTools)
set.seed(123)
split = sample.split(news_train1x, SplitRatio = 0.80)
 
training_set = subset(news_train1x, split == TRUE)
test_set = subset(news_train1x, split == FALSE)

```



```{r}
copy = test_set
```


```{r}
test_set = test_set[,!names(test_set) %in% c("Labels")]
```

```{r}
Labels = copy$Labels 

```


# SVM

```{r}
#install.packages("e1071")
#install.packages("kernlab")
library(kernlab)
library(e1071)
svm_classifier=ksvm(Labels~.,data=training_set)
```

```{r}
y_pred= predict(svm_classifier,test_set )
```


```{r}
library(caret)
cm=confusionMatrix(y_pred,Labels)
cm
```

```{r}
acc=y_pred==Labels
acc=sum(acc)
acc=(acc/nrow(test_set))*100
paste("Non - Linear SVM Accuracy : ",acc,"%")
cm = as.matrix(table(Actual = Labels, Predicted = y_pred))
 n = sum(cm) # number of instances
 nc = nrow(cm) # number of classes
 diag = diag(cm) # number of correctly classified instances per class 
 rowsums = apply(cm, 1, sum) # number of instances per class
 colsums = apply(cm, 2, sum) # number of predictions per class

 
recal=diag/rowsums
prece = diag/colsums
f1= 2*prece*recal/(prece+recal)

```



```{r}
data.frame(precision= prece, recall= recal, f1=f1)
```




```{r}
cm
prece = diag/colsums
prece

```



# KNN

```{r}
# Installing Packages
#install.packages("ClusterR")
#install.packages("cluster")

# Loading package
library(ClusterR)
library(cluster)
```

```{r}
memory.limit(size= 560000)
```



```{r}
#install.packages("class")
library(e1071)
library(caTools)
library(class)
knn_classifier = knn(train = training_set , test = copy , cl= training_set$Labels , k = 2)
```


```{r}
library(caret)
cm=confusionMatrix(Labels,knn_classifier)
cm
```


```{r}
acc=knn_classifier==Labels
acc=sum(acc)
acc=(acc/nrow(test_set))*100
paste("KNN Accuracy : ",acc,"%")
cm = as.matrix(table(Actual = Labels, Predicted = knn_classifier))
n = sum(cm) # number of instances
nc = nrow(cm) # number of classes
diag = diag(cm) # number of correctly classified instances per class 
rowsums = apply(cm, 1, sum) 
colsums = apply(cm, 2, sum)

 
 
recal=diag/rowsums
prece = diag/colsums
f1= 2*prece*recal/(prece+recal)

data.frame(precision= prece, recall= recal, f1=f1)
```
#Accuracy 

```{r}
accuracy = c(76.09,88.11,77.06,56.0,98)
model = c("Non Linear SVM","KNN","BERT","TCN","LSTM")
barplot(accuracy,names.arg=model,xlab="Models",ylab="Accuracy %",col=c("red","pink","orange","purple"),main="Accuracy Comparison",border="red",width = c(0.1,0.1 ,0.1,0.1))
```


#Recall


```{r}
df1 <- data.frame(x = c(1,0.0,1,0.502,0.78,0.68,0.78,0.69,0.99,0.96), 
                   grp = rep(c("Non-Linear SVM", "KNN",
                               "BERT","TCN","LSTM"),
                               each = 2),
                   subgroup = c("Negative"," Positive"))
# Modifying data
df1 <- reshape(df1,idvar = "subgroup",
               timevar = "grp",
               direction = "wide")
  
row.names(df1) <- df1$subgroup
df1 <- df1[ , 2:ncol(df1)]
colnames(df1) <- c("Non-Linear SVM", "KNN", "BERT","TCN","LSTM")
df1 <- as.matrix(df1)
  
# Create grouped barplot
barplot(height = df1,beside = TRUE,col= c("orange","blue"),main="Recall Comparison",xlab="Models",ylab="Recall ",border="red",legend = TRUE,args.legend = list(x = "topleft", inset = c(0.09, -0.15)))

```




# Precision

```{r}
df2 <- data.frame(x = c(0.760,NaN,0.864,1,0.71,0.76,0.77,0.70,0.97,0.99), 
                   grp = rep(c("Non-Linear SVM", "KNN",
                               "BERT","TCN","LSTM"),
                               each = 2),
                   subgroup = c("Negative"," Positive"))
# Modifying data
df2 <- reshape(df2,idvar = "subgroup",
               timevar = "grp",
               direction = "wide")
  
row.names(df2) <- df2$subgroup
df2 <- df2[ , 2:ncol(df2)]
colnames(df2) <- c("Non-Linear SVM", "KNN", "BERT","TCN","LSTM")
df2 <- as.matrix(df2)
  
# Create grouped barplot
barplot(height = df2,beside = TRUE,col= c("orange","blue"),main="Precision Comparison",xlab="Models",ylab=" Precision",border="red",legend. = TRUE,args.legend = list(x = "topleft", inset = c(0.09, -0.15)))

```

# F1 Score

```{r}
gfg <- data.frame(x = c(0.864,NaN,0.927,0.669,0.74,0.72,0.78,0.70,0.98,0.97), 
                   grp = rep(c("Non-Linear SVM", "KNN",
                               "BERT","TCN","LSTM"),
                               each = 2),
                   subgroup = c("Negative"," Positive"))
# Modifying data
gfg <- reshape(gfg,idvar = "subgroup",
               timevar = "grp",
               direction = "wide")
  
row.names(gfg) <- gfg$subgroup
gfg <- gfg[ , 2:ncol(gfg)]
colnames(gfg) <- c("Non-Linear SVM", "KNN", "BERT","TCN","LSTM")
gfg <- as.matrix(gfg)
  
# Create grouped barplot
barplot(height = gfg,beside = TRUE,col= c("orange","blue"),main="F1 Score Comparison",xlab="Models",ylab="F1 Score",border="red",legend. = TRUE,args.legend = list(x = "topleft", inset = c(0.09, -0.15)))

```
```

